{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Inteligencia Artificial y Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Introducción al Procesamiento Superficial de Textos</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Latent Dirichlet Allocation - LDA</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Análisis superficial de textos](#Análisis-superficial-de-textos)\n",
    "* [Terminología general](#Terminología-general)\n",
    "* [Preprocesamiento de datos textuales](#Preprocesamiento-de-datos-textuales)\n",
    "* [TF-IDF](#TF-IDF)\n",
    "* [Semántica latente](#Semántica-latente)\n",
    "* [Modelos generativos: Latent Dirichlet Allocation](#Modelos-generativos:-Latent-Dirichlet-Allocation)\n",
    "* [Ejemplo:Un millón de titulares](#Ejemplo:-Un-millón-de-titulares)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los humanos nos  comunicamos utilizando lenguajes naturales. Los lenguajes naturales difieren de los lenguajes de programación en que éstos últimos siguen reglas sintácticas y semánticas extrictas, mientras que los primeros por su complejidad dependen del contexto.\n",
    "\n",
    "\n",
    "En general el análisis de textos tiene dos grandes subáreas: el análisis superficial de textos y el procesamiento del lenguaje natural.\n",
    "\n",
    "En esta lección nos ocupamos del análisis superficial de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Análisis superficial de textos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta subárea se desarrolló primero, debido a que los problemas asociados al lenguaje natural en este caso son mas simples. Se trata de técnicas en la cuales se busca encontar los tópicos subayacentes en los texto. En este sentido, son modelos de tipo no supervisado y consecuencia basados en técnicas de clasificación automática. \n",
    "\n",
    "Estas técnicas están orientadas a detectar clusters de palabras y documentos en grandes corpus de datos.\n",
    "\n",
    "Un documento es en este caso una unidad distinguible de otras en el corpus. Por ejemplo una respuesta abierta en una encuesta, un comentario en una revisión, un abstract de un documento, etc. \n",
    "\n",
    "Luego de omitidos términos que se considera que no aportan a la detección de tópicos (temáticas), usualmente conocidos como *palabras vacías* (`stop words`) y de otros procesos de preprocesamiento como lematización, recorte (`steeming`),  es común construir una matriz denominada documento-témino (`dtm`).\n",
    "\n",
    "Esta matriz `dtm` representa por las filas a cada uno de los documentos individuales del corpus y por las columnas a cada uno de los términos conservados en el análisis. Cada posición de la matriz contiene el número de veces que un término aparece en el documento. En algunos casos esta es una matriz binaria, en cuyo caso la dtm indica cuando un término aparece en un documento.\n",
    "\n",
    "La  `dtm` es la base de las técnicas conocidas genéricamente como *bolsa de palabras* (`word-bag`). El nombre deriva del hecho de que al organizar la dtm, el contexto de las palabras en cada documento se pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Terminología general</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras o términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra es la unidad mínima de información  en el trabajo con lenguaje natural. \n",
    "\n",
    "Desde una perspectiva muy moderna las palabras son objetos que puede pensarse como puntos que están en un espacio de alta dimensión, de tal manera que, puntos cercanos en algún sentido de distancia corresponde a palabras que tienen una cercanía dentro de un universo de palabras considerado.\n",
    "\n",
    "La siguiente imagen corresponde a uno conjunto de palabras de astrofísica, consideradas en un estudio de resumenes de artículos científicos. Este es un gráfico obtenido luego de un procesamiento como lo que mostramos hoy, desarrollado por Montenegro y Montenegro usando una técnica de análisis basada en la teoría de respuesta al ítem multidimensional (TRIM).\n",
    "\n",
    "En este documeneto las palabaras se denotarán como $w_i, i =1,2,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cluster_kmeans_10.png\" width=\"700\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Areas de conocimiento Astrofísica, a partir de artículos científicos</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los documentos son los sujetos en los análisis textual superficial. Suponemos que se tiene un conjunto de documentos individuales, cada uno de los cuales se denotará por $\\mathbf{w}$. Se considera que un documento es una sucesión  de $N$ palabras. Así se tiene que un documento se denota como $\\mathbf{w} = \\{w_1,\\ldots,w_N \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un corpus es una colección de documentos en un problema particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tópicos son áreas latentes a las cuales están asociados tanto las palabras como los documentos. Uno de los propósitos principales del análisis de textos es descubrir o poner en evidencia tales tópicos.\n",
    "\n",
    "La figura anterior muestra por ejemplo la presencia de 10 tópicos en el conjunto de documentos de astrofísica analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Preprocesamiento de datos textuales</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que sigue, vamos a utilizar los términos token y tokenizar, que aún no son adoptados por la Real Academia de la Lengua, pero que creemos pronto lo serán como tantos otros provenientes del inglés debido a su enorme utilización actual, por razón de los desarrollos científicos y tecnológicos. \n",
    "\n",
    "Realizaremos los siguientes pasos:\n",
    "    \n",
    "- **Tokenización**: divide el texto en oraciones y las oraciones en palabras. Ponga las palabras en minúsculas y elimine la puntuación.\n",
    "- Se **eliminan las palabras que tienen menos de 3 caracteres**.\n",
    "- Se eliminan todas las **palabras vacías**.\n",
    "- Las palabras se **lematizan**: las palabras en tercera persona se cambian a primera persona y los verbos en tiempo pasado y futuro se cambian a presente.\n",
    "- Las palabras se recortan (**stemming**): las palabras se reducen a su forma raíz.\n",
    "\n",
    "Usaremos  las librerías *gensim* y *nltk* para hacer este trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos términos que se utilizarán con frecuencia son:\n",
    "\n",
    "- `Corpus`: cuerpo del texto, singular. Corpora es el plural de corpus.\n",
    "- `Léxico`: palabras y sus significados.\n",
    "- `Token`: cada *entidad* que es parte de lo que sea que se dividió según las reglas que establecemos apara el análisis. Por ejemplo, cada palabra es un token cuando una oración se tokeniza en palabras. Cada oración también puede ser un token, si ha convertido las oraciones en un párrafo.\n",
    "\n",
    "Básicamente, tokenizar implica dividir oraciones y palabras del cuerpo del texto.\n",
    "\n",
    "Veá el siguiente ejemplo tomado de [Geek for Geeks](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/?ref=rp). Usamos la librería *nltk*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa recursos de `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing  \n",
    "# libraries \n",
    "import nltk\n",
    "\n",
    "\n",
    "# tokenizadores\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# diccionarios especiales para puntuación y palabras vacias\n",
    "nltk.download('punkt') # Manejo de puntuación\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lematizador basado en WordNet de nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# steemer de nltk. Raiz de las palabras\n",
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa recursos de `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar tweets usando  los parámetros `strip_handles` y `reduce_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "reviews_token = []\n",
    "reviews_token.clear()\n",
    "for i in df_reviews[\"Reviews\"][:]:\n",
    "    review = ''.join(i)\n",
    "    result = tknzr.tokenize(review)\n",
    "    reviews_token.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cambiar texto a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(reviews_token)): \n",
    "    reviews_token[k][:] = [j.lower() for j in reviews_token[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reviews_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve carateres especiales - expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las expresiones regulares son objetos matemáticos que permiten interpretar trozos de texto. Son claves en la construcción de los lenguajes de programación. Aquí vamos a usar la librería [re](https://docs.python.org/3/library/re.html) de Python creada para el manejo de expresiones regulares. Les sugerimos este [tutorial sobre re en Python](https://www.w3schools.com/python/python_regex.asp) para aprender a manejar la librería re.\n",
    "\n",
    "Usaremos aquí para eliminar algunos símbolos: los números y los paréntesis por ejemplo. No siempre es el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# digitos\n",
    "\n",
    "for k in range(len(reviews_token)): \n",
    "    reviews_token[k][:] = [re.sub(r'\\d+', '',j) for j in reviews_token[k]]\n",
    "# parentecis\n",
    "for k in range(len(reviews_token)): \n",
    "    reviews_token[k][:] = [re.sub(r'[()]', '',j) for j in reviews_token[k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tuve', 'la', 'experiencia', 'de', 'ir', 'almozar', 'a', 'este', 'típico', 'restaurante', 'colombiano', 'en', 'el', 'centro', 'de', 'bogota', '.', 'sus', 'platos', 'están', 'deliciosos', 'y', 'además', 'la', 'mezcla', 'de', 'sabores', 'de', 'esta', 'cultura', 'no', 'se', 'puede', 'dejar', 'de', 'probar', '.'], ['el', 'restaurante', 'es', 'agradable', ',', 'antiguo', 'y', 'familiar', '.', 'en', 'lo', 'personal', 'me', 'agradó', 'bastante', ',', 'el', 'servicio', 'es', 'bueno', 'y', 'su', 'comida', 'típica', 'colombiana', 'está', 'muy', 'bien', 'lograda', '.', 'es', 'un', 'clásico', 'del', 'tour', 'gastronómico', 'de', 'bogotá', ',', 'recomendado', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(reviews_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve palabras de longitud menor o igual a tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_token_cm = []\n",
    "reviews_token_cm.clear()\n",
    "for k in range(len(reviews_token)): \n",
    "    rev_list = []\n",
    "    rev_list.clear()\n",
    "    for i in reviews_token[k]:       \n",
    "        if len(i) > 3:\n",
    "            rev_list.append(i)\n",
    "    reviews_token_cm.append(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tuve', 'experiencia', 'almozar', 'este', 'típico', 'restaurante', 'colombiano', 'centro', 'bogota', 'platos', 'están', 'deliciosos', 'además', 'mezcla', 'sabores', 'esta', 'cultura', 'puede', 'dejar', 'probar'], ['restaurante', 'agradable', 'antiguo', 'familiar', 'personal', 'agradó', 'bastante', 'servicio', 'bueno', 'comida', 'típica', 'colombiana', 'está', 'bien', 'lograda', 'clásico', 'tour', 'gastronómico', 'bogotá', 'recomendado']]\n"
     ]
    }
   ],
   "source": [
    "print(reviews_token_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Palabras vacias (stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras varias o stop words son palabras que en el lenguaje común se considera que no aportan al contenido semántico de los textos. En la técnica de bolsa de palabras son omitidos, debido a que causan clasificaciones confusas. En realidad el concepto de palabras vacía depende del contesto de utlización de las técnicas. \n",
    "\n",
    "El siguiente ejemplo muestra el diccionario de palabras vacías del inglés contenidas en la librería `gensim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la librería *nltk* el diciconario de palabras vacías del inglés es actualmente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que los dos conjuntos de palabara vacías son distintos.\n",
    "\n",
    "Como ejemplo vamos quitar la palabras vacias del objeto text tokenizado definido arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras vacías-Español nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hube', 'sentidos', 'estarías', 'tengamos', 'teníamos', 'estuviera', 'tus', 'esté', 'tendrían', 'tuya', 'hayamos', 'tuvieras', 'hubiéramos', 'nuestras', 'habido', 'estabais', 'estuvieron', 'teniendo', 'estar', 'hubieras', 'los', 'no', 'estaríais', 'tuvierais', 'pero', 'algunas', 'mi', 'habidos', 'esos', 'fuéramos', 'vosotros', 'algo', 'estábamos', 'sois', 'será', 'y', 'algunos', 'hubieran', 'suyos', 'durante', 'antes', 'hubiese', 'tuviera', 'nuestra', 'tuvimos', 'tendrías', 'hubisteis', 'ti', 'nuestros', 'estos', 'serían', 'había', 'era', 'estaban', 'vuestro', 'fue', 'hubiera', 'habríais', 'muchos', 'mucho', 'e', 'estás', 'desde', 'habidas', 'tuyas', 'estado', 'ya', 'tuviésemos', 'estéis', 'habíamos', 'sentida', 'habré', 'habéis', 'habrán', 'tuviéramos', 'al', 'ante', 'estamos', 'habían', 'sería', 'estés', 'todos', 'teníais', 'entre', 'haya', 'estarás', 'muy', 'fuésemos', 'otros', 'tengo', 'tenías', 'estuvieseis', 'mis', 'estarán', 'fueses', 'sentidas', 'siente', 'os', 'hayas', 'otra', 'tuvieran', 'esas', 'fuera', 'suyas', 'tendrás', 'eres', 'ni', 'tengáis', 'has', 'tuyo', 'estuvieran', 'nada', 'estando', 'estuviésemos', 'te', 'nos', 'ella', 'estemos', 'hay', 'estuvieras', 'lo', 'seas', 'fui', 'estas', 'hemos', 'habríamos', 'fueseis', 'sintiendo', 'está', 'eran', 'fuesen', 'nosotras', 'tuve', 'erais', 'del', 'mí', 'serás', 'otro', 'el', 'eso', 'contra', 'esta', 'estará', 'sin', 'sobre', 'hubiésemos', 'hubieseis', 'habrás', 'nuestro', 'con', 'ellos', 'suyo', 'hubimos', 'habría', 'eras', 'ellas', 'estén', 'seréis', 'vuestros', 'unos', 'les', 'habrían', 'son', 'mía', 'tuvo', 'seré', 'este', 'tienen', 'hubiesen', 'quien', 'tuviste', 'habida', 'todo', 'él', 'nosotros', 'tenemos', 'serán', 'tu', 'estada', 'una', 'habremos', 'tuyos', 'estaba', 'un', 'sentido', 'estuviesen', 'tenía', 'en', 'las', 'tendréis', 'habíais', 'tendrá', 'tenidas', 'cuando', 'estáis', 'tenida', 'estuviéramos', 'tenéis', 'sus', 'tuvieseis', 'seríais', 'tú', 'a', 'estaré', 'vuestras', 'que', 'estuviste', 'estados', 'tendremos', 'vosotras', 'hayáis', 'tendrán', 'estaríamos', 'esto', 'también', 'seríamos', 'su', 'estaréis', 'mías', 'estuvimos', 'o', 'tenga', 'seamos', 'vuestra', 'fuiste', 'fuese', 'tendré', 'estuvo', 'se', 'tuviesen', 'poco', 'hubiste', 'tenían', 'tuviese', 'sentid', 'habías', 'tenido', 'le', 'fuimos', 'estaremos', 'habiendo', 'tengan', 'tendríais', 'fueron', 'fueras', 'uno', 'soy', 'estaría', 'éramos', 'porque', 'hubo', 'tendríamos', 'tengas', 'tiene', 'tenidos', 'tanto', 'como', 'donde', 'es', 'seremos', 'estoy', 'estuvieses', 'somos', 'sea', 'yo', 'para', 'están', 'sean', 'por', 'estarían', 'suya', 'esa', 'han', 'fuerais', 'estuvisteis', 'mío', 'me', 'qué', 'estadas', 'tened', 'habrá', 'hubieses', 'estuve', 'otras', 'habrías', 'fuisteis', 'hubieron', 'tuvieron', 'hasta', 'quienes', 'tienes', 'fueran', 'estuvierais', 'estuviese', 'tuvisteis', 'la', 'ha', 'más', 'hubierais', 'tuvieses', 'estabas', 'ese', 'hayan', 'habréis', 'cual', 'sí', 'estad', 'seáis', 'tendría', 'he', 'míos', 'de', 'serías'}\n"
     ]
    }
   ],
   "source": [
    "palabrasVacias = set(stopwords.words('spanish'))\n",
    "#\n",
    "print(palabrasVacias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a quitar las palabras vacías del ejemplo usando nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tuve', 'experiencia', 'almozar', 'este', 'típico', 'restaurante', 'colombiano', 'centro', 'bogota', 'platos', 'están', 'deliciosos', 'además', 'mezcla', 'sabores', 'esta', 'cultura', 'puede', 'dejar', 'probar'], ['restaurante', 'agradable', 'antiguo', 'familiar', 'personal', 'agradó', 'bastante', 'servicio', 'bueno', 'comida', 'típica', 'colombiana', 'está', 'bien', 'lograda', 'clásico', 'tour', 'gastronómico', 'bogotá', 'recomendado']]\n"
     ]
    }
   ],
   "source": [
    "print(reviews_token_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_clean = []\n",
    "reviews_clean.clear()\n",
    "for k in range(len(reviews_token_cm)):\n",
    "    rev_list = []\n",
    "    rev_list.clear()\n",
    "    for i in reviews_token_cm[k]:     \n",
    "        if i not in palabrasVacias:\n",
    "            rev_list.append(i)\n",
    "    reviews_clean.append(rev_list)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_str = []\n",
    "for i in reviews_clean:\n",
    "    strA = \" \".join(i)\n",
    "    reviews_str.append(strA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reviews_clean[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"reviews_limpios\"] = pd.Series(reviews_str, dtype=\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Título</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>reviews_limpios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Casa Vieja Restaurante</td>\n",
       "      <td>Comida Tiquita en un ambiente muy colombiano</td>\n",
       "      <td>15 de octubre de 2019</td>\n",
       "      <td>50</td>\n",
       "      <td>Tuve la experiencia de ir almozar a este típic...</td>\n",
       "      <td>Colombiana, Sudamericana</td>\n",
       "      <td>experiencia almozar típico restaurante colombi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Casa Vieja Restaurante</td>\n",
       "      <td>Me gustó!!</td>\n",
       "      <td>22 de octubre de 2018</td>\n",
       "      <td>40</td>\n",
       "      <td>El restaurante es agradable, antiguo y familia...</td>\n",
       "      <td>Colombiana, Sudamericana</td>\n",
       "      <td>restaurante agradable antiguo familiar persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Casa Vieja Restaurante</td>\n",
       "      <td>Restaurante tradicional frente al Hotel Tequen...</td>\n",
       "      <td>18 de septiembre de 2018</td>\n",
       "      <td>40</td>\n",
       "      <td>Llegamos tarde como a las 17 horas, ya no habí...</td>\n",
       "      <td>Colombiana, Sudamericana</td>\n",
       "      <td>llegamos tarde horas público atendieron bien c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Casa Vieja Restaurante</td>\n",
       "      <td>QUE GRATO VOLVER</td>\n",
       "      <td>4 de agosto de 2016</td>\n",
       "      <td>50</td>\n",
       "      <td>Hace años no disfrutaba de un almuerzo en este...</td>\n",
       "      <td>Colombiana, Sudamericana</td>\n",
       "      <td>hace años disfrutaba almuerzo agradable restau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Casa Vieja Restaurante</td>\n",
       "      <td>Un clásico que no defrauda</td>\n",
       "      <td>1 de agosto de 2016</td>\n",
       "      <td>40</td>\n",
       "      <td>Lugar típico de cocina colombiana que uno no p...</td>\n",
       "      <td>Colombiana, Sudamericana</td>\n",
       "      <td>lugar típico cocina colombiana puede dejar vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  Nombre  \\\n",
       "0           0  Casa Vieja Restaurante   \n",
       "1           1  Casa Vieja Restaurante   \n",
       "2           2  Casa Vieja Restaurante   \n",
       "3           3  Casa Vieja Restaurante   \n",
       "4           4  Casa Vieja Restaurante   \n",
       "\n",
       "                                              Título  \\\n",
       "0       Comida Tiquita en un ambiente muy colombiano   \n",
       "1                                         Me gustó!!   \n",
       "2  Restaurante tradicional frente al Hotel Tequen...   \n",
       "3                                   QUE GRATO VOLVER   \n",
       "4                         Un clásico que no defrauda   \n",
       "\n",
       "                      Fecha  Rating  \\\n",
       "0     15 de octubre de 2019      50   \n",
       "1     22 de octubre de 2018      40   \n",
       "2  18 de septiembre de 2018      40   \n",
       "3       4 de agosto de 2016      50   \n",
       "4       1 de agosto de 2016      40   \n",
       "\n",
       "                                             Reviews  \\\n",
       "0  Tuve la experiencia de ir almozar a este típic...   \n",
       "1  El restaurante es agradable, antiguo y familia...   \n",
       "2  Llegamos tarde como a las 17 horas, ya no habí...   \n",
       "3  Hace años no disfrutaba de un almuerzo en este...   \n",
       "4  Lugar típico de cocina colombiana que uno no p...   \n",
       "\n",
       "                       Tipo                                    reviews_limpios  \n",
       "0  Colombiana, Sudamericana  experiencia almozar típico restaurante colombi...  \n",
       "1  Colombiana, Sudamericana  restaurante agradable antiguo familiar persona...  \n",
       "2  Colombiana, Sudamericana  llegamos tarde horas público atendieron bien c...  \n",
       "3  Colombiana, Sudamericana  hace años disfrutaba almuerzo agradable restau...  \n",
       "4  Colombiana, Sudamericana  lugar típico cocina colombiana puede dejar vis...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.to_csv('restaurant_reviews_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
